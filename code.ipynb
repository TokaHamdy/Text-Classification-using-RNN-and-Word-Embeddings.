{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "D7643SOYRM-l",
    "outputId": "e86f4e23-e180-499e-8eba-07304a9df79e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#Read the CSV file\n",
    "data = pd.read_csv('amazon_reviews.csv')\n",
    "import nltk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aq-GC0BbT_HV",
    "outputId": "839e8b73-3e75-4bb4-dd34-7ac319dc50a2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Cleaning of the dataset\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Drop NULL values \n",
    "data = data.dropna() \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'<.*?>', '', text) #Remove html brackets and s\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        words = word_tokenize(text)  # Tokenize\n",
    "        filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "        return ' '.join(filtered_words)\n",
    "   \n",
    "#Apply preprocessing steps on the data\n",
    "data['cleaned_review'] = data['cleaned_review'].apply(preprocess_text)\n",
    "\n",
    "# Encode the labels\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "data['sentiments'] = data['sentiments'].map(label_mapping)\n",
    "\n",
    "#Encode review score\n",
    "# 1,2 -> Negative (0)\n",
    "data.loc[data['review_score'] < 3, 'review_score'] = 0\n",
    "\n",
    "# 3 -> Neutral (2)\n",
    "data.loc[data['review_score'] == 3, 'review_score'] = 1\n",
    "\n",
    "# 4,5 -> Positive (1)\n",
    "data.loc[data['review_score'] > 3, 'review_score'] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>cleaned_review_length</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>wish would gotten one earlier love makes worki...</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>learned lesson open package use product right ...</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>slow lags find better option</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>roller ball stopped working within months mini...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>like color size days return period hold charge</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiments                                     cleaned_review  \\\n",
       "0           2  wish would gotten one earlier love makes worki...   \n",
       "1           1  learned lesson open package use product right ...   \n",
       "2           1                       slow lags find better option   \n",
       "3           1  roller ball stopped working within months mini...   \n",
       "4           1     like color size days return period hold charge   \n",
       "\n",
       "   cleaned_review_length  review_score  \n",
       "0                     19           2.0  \n",
       "1                     88           0.0  \n",
       "2                      9           0.0  \n",
       "3                     12           0.0  \n",
       "4                     21           0.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "fpxaoeITjjDh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_review'], data['sentiments'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization \n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "X_train_pad = pad_sequences(sequences_train, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_test_cat = to_categorical(y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLY68DJUSWde",
    "outputId": "b5ae2735-f0ec-4e85-bc88-0cdacee8d901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "217/217 [==============================] - 17s 77ms/step - loss: 0.6206 - accuracy: 0.7370 - val_loss: 0.4623 - val_accuracy: 0.8356\n",
      "Epoch 2/5\n",
      "  2/217 [..............................] - ETA: 14s - loss: 0.3019 - accuracy: 0.9297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - 15s 69ms/step - loss: 0.2855 - accuracy: 0.9026 - val_loss: 0.4075 - val_accuracy: 0.8636\n",
      "Epoch 3/5\n",
      "217/217 [==============================] - 14s 66ms/step - loss: 0.1235 - accuracy: 0.9636 - val_loss: 0.4554 - val_accuracy: 0.8558\n",
      "Epoch 4/5\n",
      "217/217 [==============================] - 15s 68ms/step - loss: 0.1072 - accuracy: 0.9658 - val_loss: 0.5222 - val_accuracy: 0.8653\n",
      "Epoch 5/5\n",
      "217/217 [==============================] - 14s 66ms/step - loss: 0.0461 - accuracy: 0.9886 - val_loss: 0.5589 - val_accuracy: 0.8578\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Simple RNN model and training\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "rnn_model.add(SimpleRNN(128))\n",
    "rnn_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the RNN model\n",
    "rnn_model_checkpoint = ModelCheckpoint('rnn_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "rnn_history = rnn_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=64, validation_data=(X_test_pad, y_test_cat), callbacks=[rnn_model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCP4PcwnU7nG",
    "outputId": "aa66d6b1-4054-4584-850a-6d2e5a827063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "217/217 [==============================] - 43s 195ms/step - loss: 0.5946 - accuracy: 0.7445 - val_loss: 0.4494 - val_accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "217/217 [==============================] - 42s 192ms/step - loss: 0.3287 - accuracy: 0.8794 - val_loss: 0.3955 - val_accuracy: 0.8535\n",
      "Epoch 3/5\n",
      "217/217 [==============================] - 41s 190ms/step - loss: 0.2301 - accuracy: 0.9221 - val_loss: 0.4057 - val_accuracy: 0.8587\n",
      "Epoch 4/5\n",
      "217/217 [==============================] - 42s 193ms/step - loss: 0.1803 - accuracy: 0.9399 - val_loss: 0.4068 - val_accuracy: 0.8682\n",
      "Epoch 5/5\n",
      "217/217 [==============================] - 42s 192ms/step - loss: 0.1438 - accuracy: 0.9544 - val_loss: 0.4464 - val_accuracy: 0.8653\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# LSTM model and trining\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model_checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "lstm_history = lstm_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=64, validation_data=(X_test_pad, y_test_cat), callbacks=[lstm_model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUH-522tlkqI",
    "outputId": "8468cf30-4c64-46f2-fb71-953a2062a10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 1s 11ms/step - loss: 0.5222 - accuracy: 0.8653\n",
      "RNN Validation Accuracy: 0.8653402328491211\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on validation data\n",
    "rnn_model.load_weights('rnn_model.h5')\n",
    "lstm_model.load_weights('lstm_model.h5')\n",
    "\n",
    "rnn_val_accuracy = rnn_model.evaluate(X_test_pad, y_test_cat)[1]\n",
    "lstm_val_accuracy = lstm_model.evaluate(X_test_pad, y_test_cat)[1]\n",
    "\n",
    "print(f'RNN Validation Accuracy: {rnn_val_accuracy}')\n",
    "print(f'LSTM Validation Accuracy: {lstm_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First boint in BONUS part to predict pased on user input \n",
    "def predfict_review(review, model, tokenizer, max_len):\n",
    "    review = preprocess_text(review)\n",
    "    sequence = tokenizer.texts_to_sequences([review])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    sentiment_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    return sentiment_labels[sentiment]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a new review:  i like it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002483580A5F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002483580B910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "RNN Model Prediction: neutral\n",
      "LSTM Model Prediction: neutral\n"
     ]
    }
   ],
   "source": [
    "#Taking the input from the user and test using two model :Simple RNN and LSTM\n",
    "new_review = input(\"Enter a new review: \")\n",
    "rnn_sentiment = predfict_review(new_review, rnn_model, tokenizer, max_len)\n",
    "lstm_sentiment = predfict_review(new_review, lstm_model, tokenizer, max_len)\n",
    "\n",
    "print(f'RNN Model Prediction: {rnn_sentiment}')\n",
    "print(f'LSTM Model Prediction: {lstm_sentiment}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model best Hyperparameters:\n",
      "   Split Ratio  Padding Length  Accuracy\n",
      "2          0.6             200  0.846864\n",
      "3          0.7             100  0.860246\n",
      "6          0.8             100  0.871972\n",
      "\n",
      "Accuracy against parameters:\n",
      "Padding Length       100       150       200\n",
      "Split Ratio                                 \n",
      "0.6             0.843980  0.846575  0.846864\n",
      "0.7             0.860246  0.858324  0.856017\n",
      "0.8             0.871972  0.864187  0.871972\n"
     ]
    }
   ],
   "source": [
    "#Second point in BONUS part \n",
    "def evaluate_and_train_for_LSTM_model(ratio_of_split, lengt_of_padding):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['cleaned_review'], data['sentiments'], test_size=1-ratio_of_split, random_state=42)\n",
    "\n",
    "    # Tokenization \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    X_train_pad = pad_sequences(sequences_train, maxlen=padding_length)\n",
    "    X_test_pad = pad_sequences(sequences_test, maxlen=padding_length)\n",
    "\n",
    "    y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "    # Define and train LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(max_words, 128, input_length=padding_length))\n",
    "    lstm_model.add(LSTM(128))\n",
    "    lstm_model.add(Dense(3, activation='softmax'))\n",
    "    lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lstm_model.fit(X_train_pad, y_train_cat, epochs=4, batch_size=64, validation_data=(X_test_pad, y_test_cat), verbose=0)\n",
    "\n",
    "    # Evaluation of the model\n",
    "    accuracy = lstm_model.evaluate(X_test_pad, y_test_cat, verbose=0)[1]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "ratios = [0.6, 0.7, 0.8]\n",
    "lengths = [50, 100, 150]\n",
    "\n",
    "answers = []\n",
    "\n",
    "# Saving answers in dataframe\n",
    "for split_ratio in ratios:\n",
    "    for padding_length in lengths:\n",
    "        accuracy = evaluate_and_train_for_LSTM_model(split_ratio, padding_length)\n",
    "        answers.append({'Split Ratio': split_ratio, 'Padding Length': padding_length, 'Accuracy': accuracy})\n",
    "answers_df = pd.DataFrame(answers)\n",
    "best_hyperparameters = answers_df.loc[answers_df.groupby('Split Ratio')['Accuracy'].idxmax()]\n",
    "\n",
    "print(\"LSTM Model best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "answers_df = answers_df.pivot(index='Split Ratio', columns='Padding Length', values='Accuracy')\n",
    "answers_df.index.name = 'Split Ratio'\n",
    "print(\"\\nAccuracy against parameters:\")\n",
    "print(answers_df)\n",
    "answers_df.to_csv('lstm_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Model best Hyperparameters:\n",
      "   Split Ratio  Padding Length  Accuracy\n",
      "0          0.6             100  0.839221\n",
      "5          0.7             200  0.861207\n",
      "6          0.8             100  0.856690\n",
      "\n",
      "Accuracy against parameters:\n",
      "Padding Length       100       150       200\n",
      "Split Ratio                                 \n",
      "0.6             0.839221  0.805768  0.725306\n",
      "0.7             0.857555  0.828528  0.861207\n",
      "0.8             0.856690  0.832180  0.687140\n"
     ]
    }
   ],
   "source": [
    "#Second point in BONUS part \n",
    "def evaluate_and_train_for_simpleRNN_model(ratio_of_split, lengt_of_padding):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['cleaned_review'], data['sentiments'], test_size=1-ratio_of_split, random_state=42)\n",
    "\n",
    "    # Tokenization \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    X_train_pad = pad_sequences(sequences_train, maxlen=padding_length)\n",
    "    X_test_pad = pad_sequences(sequences_test, maxlen=padding_length)\n",
    "\n",
    "    y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "    # Train simple RNN model\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(Embedding(max_words, 128, input_length=padding_length))\n",
    "    rnn_model.add(SimpleRNN(128))\n",
    "    rnn_model.add(Dense(3, activation='softmax'))\n",
    "    rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    rnn_model.fit(X_train_pad, y_train_cat, epochs=4, batch_size=64, validation_data=(X_test_pad, y_test_cat), verbose=0)\n",
    "\n",
    "    # Evaluation of the model\n",
    "    accuracy = rnn_model.evaluate(X_test_pad, y_test_cat, verbose=0)[1]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "ratios = [0.6, 0.7, 0.8]\n",
    "lengths = [50, 100, 150]\n",
    "\n",
    "answers = []\n",
    "\n",
    "# Saving answers in dataframe\n",
    "for split_ratio in ratios:\n",
    "    for padding_length in lengths:\n",
    "        accuracy = evaluate_and_train_for_simpleRNN_model(split_ratio, padding_length)\n",
    "        answers.append({'Split Ratio': split_ratio, 'Padding Length': padding_length, 'Accuracy': accuracy})\n",
    "answers_df = pd.DataFrame(answers)\n",
    "best_hyperparameters = answers_df.loc[answers_df.groupby('Split Ratio')['Accuracy'].idxmax()]\n",
    "\n",
    "print(\"Simple RNN Model best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "answers_df = answers_df.pivot(index='Split Ratio', columns='Padding Length', values='Accuracy')\n",
    "answers_df.index.name = 'Split Ratio'\n",
    "print(\"\\nAccuracy against parameters:\")\n",
    "print(answers_df)\n",
    "answers_df.to_csv('rnn_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
